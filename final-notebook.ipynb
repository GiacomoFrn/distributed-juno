{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Distributed processing of JUNO datasets for improved energy reconstruction\n\n## Group : *Città Romanze*\n\n### Members\n\n- Pietro Cappelli\n    - e-mail: pietro.cappelli@studenti.unipd.it\n    - ID: 2058332\n- Alberto Coppi\n    - e-mail: alberto.coppi@studenti.unipd.it\n    - ID: 2053063\n- Giacomo Franceschetto\n    - e-mail: giacomo.franceschetto@studenti.unipd.it\n    - ID: 2053348\n- Nicolò Lai\n    - e-mail: nicolo.lai@studenti.unipd.it\n    - ID: 2064377 \n### Author contribution statement\n\nEach and every member of the group equally worked on the assigned project and the drafting of this notebook. ",
   "metadata": {
    "tags": [],
    "cell_id": "64bbe750-8cf5-4b80-8914-de8da1363462",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "823ba7c3",
    "execution_start": 1657283823034,
    "execution_millis": 0,
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 674
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Introduction\n\nThe Jiangmen Underground Neutrino Observatory (JUNO) is an experiment designed to study neutrino oscillations. The ability to accurately reconstruct particle interaction events in JUNO is of great importance for the success of the experiment. \n\nSeveral machine learning approaches have been applied to the vertex and the energy reconstruction, achieving the necessary level of accuracy for reaching the physical goal of JUNO. \n\nHowever, due to the experiment geometry, data needs to be significantly preprocessed before feeding machine learning models with it. Precisely, the spherical distribution of photomultipliers (PMTs) needs to be transformed into a 2-dimensional image. The currently adopted approach is a projection method that follows three guidelines:\n\n1. A single PMT occupies a single pixel in the image to minimize the information loss.\n2. The final image has a similar PMTs arrangement to the original distribution in the detector.\n3. The total amount of pixels in the image is the minimum number that satisfies the first requirement.\n\nThere are about $17600$ PMTs in the central detector of JUNO. To avoid overlap of PMTs, these are arranged layer by layer from the top of the detector into the image's pixel so that PMTs with the same latitude will be arranged in the same row.\n\nThe optimal image size that ensures that no PMTs are overlapped is $230 \\times 124$. An example of event on the sphere and its projection into an image is shown in the figure below.\n\n<div class=\"container\" style=\"display: inline-block;\">  \n  <figure>\n  <div style=\"float: left; padding: 10px;\">\n    <img src='figures/ev_id_12_3d_.png' width=\"450\" height=\"350\" align=\"center\"/>\n    <figcaption align=\"center\"><b><br>Fig. 1: Collected charge - 3D representation</b></figcaption>\n  </div>\n  <div style=\"float: right; padding: 10px;\">\n    <img src='figures/ev_id_12_2d_.png' width=\"550\" height=\"350\" align=\"center\"/>\n    <figcaption align=\"center\"><b><br>Fig. 2: Collected charge - 2D projection</b></figcaption>\n  </div>\n  </figure>\n</div>\n\nOne of the few projection pitfalls is the high probability of having an event right where the latitude rings are opened and unrolled onto the image, as shown in the following example.\n\n<div class=\"container\" style=\"display: inline-block;\">  \n  <figure>\n  <div style=\"float: left; padding: 10px;\">\n    <img src='figures/ev_id_16_3d_.png' width=\"450\" height=\"350\" align=\"center\"/>\n    <figcaption align=\"center\"><b><br>Fig. 3: Collected charge - 3D representation</b></figcaption>\n  </div>\n  <div style=\"float: right; padding: 10px;\">\n    <img src='figures/ev_id_16_2d_.png' width=\"550\" height=\"350\" align=\"center\"/>\n    <figcaption align=\"center\"><b><br>Fig. 4: Collected charge - 2D projection</b></figcaption>\n  </div>\n  </figure>\n</div>\n\nWe believe that the occurrence of these events, that do not lead to a clear distributions of the measured observables, affects the training and prediction process of machine learning models. Therefore, the energy and vertex reconstruction accuracy could technically be improved if these events do not occur at all, and the machine learning model is fed with only clear images.\n\nHowever, being the probability of neutrino interactions ideally uniform inside the detector, the occurrence of events which lead to a splitted distribution after the projection is non-negligible. Therefore, discarding these kind of events would lead to an overall loss of information. \n\nWe here propose a novel preprocessing technique to eliminate dangerous events without being forced to ignore them, but instead turn them into clear images containing well-distributed information.\n\n",
   "metadata": {
    "cell_id": "bc2ba1f96cbf45c5893d7def1e14cac1",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1615
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Method",
   "metadata": {
    "cell_id": "911479e066de42969d6e34e1adf3351f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Rotation and Mapping\nWe want to feed machine learning models with 2D images where the the center of charge is in the middle of it and not splitted by the 2D-mapping process. In order to do it, we rotate the original sphere to prepare it for the 2D-mapping process. We operate a *Rotation* in spherical coordinates along the angles $\\theta$ and $\\varphi$, corrensponding to a rotation along the y and z axis (in this particular order). To evaluate the rotation angles ($\\theta_R$ and $\\varphi_R$) we first calculate the charge center cartesian coordinates by a weighted sum of the position of each PMT in the i-th event and the deposited charge in that PMT, normalized by the total charge deposited in that event.\n$$\n\\begin{aligned}\nx_c = \\frac{\\sum_i x_i \\cdot c_i }{\\sum_i c_i} \\, ; &&&\ny_c = \\frac{\\sum_i y_i \\cdot c_i }{\\sum_i c_i} \\, ; &&\nz_c = \\frac{\\sum_i z_i \\cdot c_i }{\\sum_i c_i}.\n\\end{aligned}\n$$\nThen we evaluate the angles by taking the spherical coordinates:\n$$\n\\begin{cases}\n\\theta_C = \\arctan  \\sqrt{\\frac{x_c^2 + y_c^2}{z_c}} \\\\\n\\varphi_C = \\arctan (\\frac{y}{x})\n\\end{cases}\n$$\nThis angles describe the vector $\\vec{r}(R,\\theta_c, \\varphi_c)$ in spherical coordinates which point in the direction of the charge center. \n\nOnce we have these angles we can operate a Rotation to bring the charge center where we want in order to achieve our goal. As we want to bring the charge center in the \"origin\" of the spherical coordinates sistem, in order to have it in the center of the images, the rotation angles will be $\\varphi_R = - \\varphi_c$ and $\\theta_R=-\\theta_c + \\frac{\\pi}{2}$.\nFinally we rotate the original sphere by evaluating the new cartesian coordinates of each PMT per events, with the following matrix multiplication:\n$$\n\\begin{bmatrix}\nx'\\\\\ny'\\\\\nz'\n\\end{bmatrix}\n=\nR_{yz}\n\\times\n\\begin{bmatrix}\nx\\\\\ny\\\\\nz\n\\end{bmatrix}\n$$\nwhere the rotation matrix is:\n$$\n{\\displaystyle R_{yz}(\\theta, \\varphi)={\\begin{pmatrix}\\cos \\varphi \\cos \\theta &\\sin \\theta \\cos \\theta &\\sin \\theta \\\\\n\\sin \\varphi &\\cos \\varphi & 0 \\\\\n-\\sin \\theta \\cos \\varphi  &\\sin \\varphi \\sin \\theta & \\cos \\theta \\end{pmatrix}}.}\n$$\nIn this way we evaluate a new dataset for each event (since we process one event at time) which contain 5 feature for each PMT which is turned on in that event. The 5 feaure are the new cartesian position, R, charge, hit time for each PMT.\n\nAfter evaluating this new dataset we proceed with the *Mapping* procedure which correspond to project the 3D sphere images into a 2D images, as described in the previous section . This final images will have the center of charge in the middle of it after the rotation process. \n",
   "metadata": {
    "cell_id": "a8f4ff73465848ccb01d19b2050dbc1f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 776
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation",
   "metadata": {
    "cell_id": "3e0349efee7f490f98b812c2fecd2790",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0dc4504e2e494ebcaf8ddeaadadd3ebe",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 282
   },
   "source": "import sys\nimport dask\nimport glob\n\nimport pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt\nimport dask.dataframe    as dd\nimport dask.bag          as db\n\nfrom dask.distributed import Client, SSHCluster\nfrom time import time, sleep\nfrom tqdm import tqdm",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Resources and Cluster\n\nWe have 5 virtual machines available on CloudVeneto, all sharing the following specifications:\n\n* VCPUs &rarr; 4\n* RAM &rarr; 8 GB\n* STORAGE &rarr; 25 GB\n\nOur datasets are stored in a separate volume of 5 TB capacity, attached to one of the five virtual machines that we will refer to as VM1 in the following. To make the datasets avaiable to all five machines, we have shared the volume through a network file system (NFS) using VM1 as NFS server and having the other four acting as NFS clients. \n\nOn top of that, we created a Dask cluster where VM1 hosts the scheduler and the other four virtual machines are employed for spawning worker processes. \n\nThis way, VM1 acts as NFS server and Dask scheduler and it is not employed in computations. Instead VM2 to VM5 are only exploited for computations. Since we are not dealing with extremely demanding tasks, it could have been possible to use VM1 for computations too. ",
   "metadata": {
    "cell_id": "4f5c00970b9f469995dd5e5a95e7aaa9",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 360
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8e250f2f1f4e4df1aa14639f1edb4965",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 318
   },
   "source": "# number of workers to spawn in each VM (VMs 2 to 5)\nn_workers_vm = 4\n\n# number of threads per worker\nn_threads_wk = 1\n\n# total number of workers is 4 virtual machines times the number of workers per machine\nn_workers    = 4 * n_workers_vm\n\ncluster = SSHCluster(\n    [\"10.67.22.39\", \"10.67.22.74\", \"10.67.22.27\", \"10.67.22.91\", \"10.67.22.60\"],\n    connect_options   = {\"known_hosts\": \"/root/.ssh/known_hosts\"},\n    worker_options    = {\"nthreads\": n_threads_wk, \"n_workers\": n_workers_vm},\n    scheduler_options = {\"dashboard_address\": \":8787\"}\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "051707b3cc574a56b7f83b5566ed3d20",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 84
   },
   "source": "client = Client(cluster)\nclient",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Load Mapping Data\n\nWith \"mapping data\" we refer to all pieces of information that are necessay to map our datasets into a 2-dimensional image, such as PMT coordinates on the sphere.",
   "metadata": {
    "cell_id": "4b6d21388d6b4486965698ed878bc9cc",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 122
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "87a9e8903ab64679bcd5536fd7450a62",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 282
   },
   "source": "# path to data\ndata_folder       = \"/root/data/data/real/train/data/\"\n\n# mapping data file names\npmt_pos_fname     = \"PMTPos_CD_LPMT.csv\"\npmt_id_conv_fname = \"PMT_ID_conversion.csv\"\n\n# data file name for one-file tests\ntrain_data_fname  = \"raw_data_train_4.npz\"\n\n# data file names for multiple-file tests\nnfiles = 4\nname_list = glob.glob('*.npz', root_dir=data_folder)[:nfiles]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "098ad019511a43338121dfba4b7dc927",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 102
   },
   "source": "# load mapping data into pandas.DataFrame\npmt_positions  = pd.read_csv(pmt_pos_fname)\nconversion_ids = pd.read_csv(pmt_id_conv_fname)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Distributed processing\n",
   "metadata": {
    "cell_id": "4007c34fc7854aa58db62b7dce843398",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "markdown",
   "source": "Our goal is to parallelize the process of the Rotation and Mapping of the events. Our approach is to process one file at time, which contain a variable number of events, by processing the single events. First of all we load one file in a `dask.bag` and split it across all the workers. This means that one worker will have a certain number of events according the number of workers and to the number of partitions of the Bag. \nThen, we build two function which Rotate and Map one single event: `rotate_ev()`, `ti_sta_scomparendo_il_tanga()`, `shakerando()`, `mapping_single_events()`. \nOur idea is to parallelize this two function in order to have each worker processing their events at the same time, through a db.map() method in Dask. First we *map* the Rotate function on the Bag with the events per worker, and then we *map* the Mapping function on the Bag with the rotated results. Recall that each operation on the Bags in Dask \"Lazy\" which means that the intermediate results are the *promise* of it. The workers start effectively to calculate the results when we call the compute method of the Bag with the mapped results: `mapped.compute()`.\n\nFor each process we also print the execution time to highlights the lazy, the eager and the compute differences.",
   "metadata": {
    "cell_id": "bdeb6d95aa39493eb325527e1dac5b17",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 242
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Defining Function",
   "metadata": {
    "cell_id": "1f54e811d34e48c7a3f88a178fd95f9f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "95f8b546f61343868a27f1315f0b7555",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 372
   },
   "source": "def load(path):\n    return np.load(path, allow_pickle=True)[\"a\"]\n\ndef R_yz(theta_rot, phi_rot):\n    return np.array([[np.cos(phi_rot) * np.cos(theta_rot), -np.sin(phi_rot) * np.cos(theta_rot), np.sin(theta_rot)], \n                     [np.sin(phi_rot), np.cos(phi_rot), 0], \n                     [-np.sin(theta_rot) * np.cos(phi_rot), np.sin(theta_rot) * np.sin(phi_rot), np.cos(theta_rot)]])\n\ndef convert_pmt_ids(input_ids, conversion_ids):\n    cd_ids  = np.array(conversion_ids[\"CdID\"])\n    pmt_ids = np.array(conversion_ids[\"PMTID\"])\n    mask    = np.isin(cd_ids, input_ids)\n    return pmt_ids[mask]\n\ndef find_pmt_coord(pmt_positions, data_pmt_id):\n    return pmt_positions[\n        np.isin(pmt_positions.PMTID, data_pmt_id)\n        ].loc[:, ['x', 'y', 'z']].reset_index(drop=True).to_numpy()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "27a7aa5be7c44bfd87ce2965a482131c",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 120
   },
   "source": "def load_bag(path, Nevents):\n    data_np = load(path)\n    data_np = data_np[:, :Nevents]\n    return [np.vstack([ data_np[j, i] for j in range(3)]) for i in range(data_np.shape[1])]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Rotate One Single Event Function",
   "metadata": {
    "cell_id": "731708bb3f2c41948995e63fcc59f494",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "65e4ae001aa643dd9652cb6f585c323f",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 732
   },
   "source": "def rotate_ev(data):\n\n    nonzeros_inds = data[2] != 0.0\n    data_pmt_id   = convert_pmt_ids(data[0][nonzeros_inds], conversion_ids)\n    pmt_coord     = find_pmt_coord(pmt_positions, data_pmt_id)\n\n    tot_charge = sum(data[1][nonzeros_inds])\n    x_cc       = sum(pmt_coord[:,0] * data[1][nonzeros_inds]) / tot_charge\n    y_cc       = sum(pmt_coord[:,1] * data[1][nonzeros_inds]) / tot_charge\n    z_cc       = sum(pmt_coord[:,2] * data[1][nonzeros_inds]) / tot_charge\n\n    theta_cc   = np.arctan2(\n        np.sqrt((x_cc)**2+(y_cc)**2), z_cc\n    )\n    phi_cc     = np.arctan2(y_cc, x_cc) \n\n    theta_rot = -theta_cc + np.pi/2\n    phi_rot   = -phi_cc\n    \n    # coord_new = np.matmul(R_yz(theta_rot, phi_rot), pmt_coord.T)\n    coord_new = np.matmul(\n        R_yz(theta_rot, phi_rot), pmt_coord.T\n    )\n\n    R = np.sqrt(np.sum(np.power(coord_new, 2), axis=0))\n\n    charge_hitt = np.vstack([data[1], data[2]])\n    charge_hitt = charge_hitt[:,nonzeros_inds]\n\n    rotated = np.vstack([coord_new, R, charge_hitt])\n    \n    del coord_new\n    del charge_hitt\n    del pmt_coord\n    del data_pmt_id\n    del nonzeros_inds\n    \n    return rotated",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Mapping One Single Event Function",
   "metadata": {
    "cell_id": "439b3fe0fa70482fa360120f2d74da09",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6b1b6a41cb6647aeb90327e8fb503a34",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 714
   },
   "source": "N_max = 115\n\ndef mapping_single_event(rotated_ev):\n    ####################\n    # rotated_ev must be computed\n    ####################\n    coord_new   = rotated_ev[:3]\n    charge_hitt = rotated_ev[4:, ].T\n    R           = rotated_ev[3, ].mean()\n    del rotated_ev\n\n    z_levels, step = np.linspace(coord_new[2,].min(), coord_new[2,].max(), 124, retstep=True)\n    image_mat      = np.zeros((230,124,2))\n\n    for j, z in enumerate(z_levels):\n        mask = (np.abs(coord_new[2,] - z) < step/2)         \n        if(not np.any(mask)): continue\n        masked = coord_new[:,mask]\n\n        Rz = (R**2 - z**2)\n        Neff = 0 if Rz < 0 else N_max * np.sqrt(Rz) / R\n        ix = np.around( Neff * (np.arctan2(masked[1,], masked[0,]) / np.pi) + (N_max / 2) ) + 57\n        ix = ix.astype(np.int32)\n        if(np.any(ix >= 230)):\n            ix[ix >= 230] = ix[ix >= 230] - 230\n\n        image_mat[ix, j,] = charge_hitt[mask, ]\n\n        del ix\n        del masked\n        del mask\n                \n    del z_levels\n    del coord_new\n    del charge_hitt\n    \n    return image_mat",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "46b13a2ef2ed4fe6ac834d8845295cd4",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 210
   },
   "source": "Nevents = None\n\n# Loading data into a Dask Bag\nstart = time()\ndata_db = db.from_sequence(load_bag(data_folder + train_data_fname, Nevents=None), npartitions=16)\nstop = time()\n\nprint('Execution time:', stop-start)\ndata_db",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4ad43962cb3649faad1d750b62b4976c",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 192
   },
   "source": "# Distribute the Rotation Process\nstart = time()\nrotated = db.map(rotate_ev, data_db)\nstop = time()\n\nprint('Execution time:', stop-start)\n\nrotated.visualize()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Plot One Rotate Event",
   "metadata": {
    "cell_id": "d15c18e3892c48789f0f6b0a4360f764",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c2fbf8a68b5743ebb7725a69a976ef73",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 264
   },
   "source": "ev = 4\nrot = rotated.take(ev+1)[ev]\n\nfig = plt.figure(figsize=(10,12))\naxr = fig.add_subplot(111, projection='3d')\n\naxr.scatter(\n    xs = rot[1, :],\n    ys = -rot[0, :],\n    zs = rot[2, :],\n    c  = rot[4, :]\n    )",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d7215ebd20b34783b3b62ed9260d8c4d",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 192
   },
   "source": "# Distribute the Mapping Process\n \nstart = time()\nmapped = db.map(mapping_single_event, rotated)\nstop = time()\n\nprint('Execution time:', stop-start)\nmapped.visualize()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00e3b385fdb3434c82c3c5964c0e2ac2",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 174
   },
   "source": "# Compute\n\nstart = time()\nimages = mapped.compute(optimize_graph=False)\nstop = time()\n\nprint('Execution time:', stop-start)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Plot One Mapped Event",
   "metadata": {
    "cell_id": "5ea6a7e87abf44ee8b77c840ed91a64a",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ed0eecccbe6f4d55be011e3ef4f2469e",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 192
   },
   "source": "ev = 4\n\nimage = images[ev][:,:,0].T\nimage[image == 0] = np.NaN\n\nfig, ax = plt.subplots(1, figsize=(20,10))\nax.imshow(image)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Benchmarks\n\nUsing the previous code, we have benchmarked the algorithm performance varying three key parameters:\n\n1. The number of workers per virtual machine\n2. The number of threads per worker\n3. The number of partitions of the Dask bag holding our dataset\n\nThe python code we have written to explore this parameter space can be found in the `benchmark.py` file. Anyway, it is simply a for loop over some values for the parameters. \n\nWe have chosen to test the time performance on 1024 events.\n\n### Heatmaps\n\nWe could only test two parameters at the same time, not three, due to workers RAM overload. Thus, we first explored `n_workers` versus `n_threads` keeping the number of partitions fixed to 16. \n\n| | Workers versus Threads - Partitions fixed to 16 | |\n|:-------------------------------------:|:-------------------------------------------:|:-----------------------------------------:|\n![](figures/heatmaps/load_time_wt.png)  |  ![](figures/heatmaps/compute_time_wt.png)  |  ![](figures/heatmaps/total_time_wt.png)\n\nThen, we tested `n_workers` versus `n_partitions` keeping the number of threads fixed to 1 thread per worker.\n\n| | Workers versus Partitions - Threads fixed to 1 | |\n|:-------------------------------------:|:-------------------------------------------:|:-----------------------------------------:|\n![](figures/heatmaps/load_time_wp.png)  |  ![](figures/heatmaps/compute_time_wp.png)  |  ![](figures/heatmaps/total_time_wp.png)\n\nFinally, to verify that the two configurations\n\n* 1 worker per VM with 4 threads\n* 4 workers per VM with 1 thread each\n\nare time-performance equivalent, we also tested these configurations with the appropriate number of partitions, namely 4 in the former configuration and 16 in the latter. Thus, we explored the time performance varying the number of workers per VM and the number of threads per worker, setting the number of partitions to the total number of workers (i.e. 4 times the number of workers per VM).\n\n| | Workers versus Threads - Partitions equal to the total number of workers | |\n|:--------------------------------------:|:--------------------------------------------:|:------------------------------------------:|\n![](figures/heatmaps/load_time_wtp.png)  |  ![](figures/heatmaps/compute_time_wtp.png)  |  ![](figures/heatmaps/total_time_wtp.png)\n\nThe results are not quite what we were expecting, as it is still present a sensible difference between the two time performance. Our first attempt at explaining why the two configurations, that should exploit the total number of VCPUs available, do not perform equally is based on the fact that Dask bags are not optimized for multi-threading if operations are pure Python functions.\n\n### Workers versus Threads trends\n\n#### Load time\n\n| Workers versus Threads - Partitions fixed to 16 | Workers versus Threads - Partitions equal to the total number of workers | \n|:-------------------------------------:|:-------------------------------------------:|\n![](figures/trends/load_time_wt_wtrend.png)  |  ![](figures/trends/load_time_wtp_wtrend.png)  \n![](figures/trends/load_time_wt_ttrend.png)  |  ![](figures/trends/load_time_wtp_ttrend.png) \n\n#### Compute time\n\n| Workers versus Threads - Partitions fixed to 16 | Workers versus Threads - Partitions equal to the total number of workers | \n|:-------------------------------------:|:-------------------------------------------:|\n![](figures/trends/compute_time_wt_wtrend.png)  |  ![](figures/trends/compute_time_wtp_wtrend.png)  \n![](figures/trends/compute_time_wt_ttrend.png)  |  ![](figures/trends/compute_time_wtp_ttrend.png) \n\n#### Total time\n\n| Workers versus Threads - Partitions fixed to 16 | Workers versus Threads - Partitions equal to the total number of workers | \n|:-------------------------------------:|:-------------------------------------------:|\n![](figures/trends/total_time_wt_wtrend.png)  |  ![](figures/trends/total_time_wtp_wtrend.png)  \n![](figures/trends/total_time_wt_ttrend.png)  |  ![](figures/trends/total_time_wtp_ttrend.png) \n\n\n### Workers versus Partitions trends (threads fixed to 1 per worker)\n\n#### Load time\n\n| Workers trend | Partitions trend | \n|:-------------------------------------:|:-------------------------------------------:|\n![](figures/trends/load_time_wp_wtrend.png)  |  ![](figures/trends/load_time_wp_ptrend.png)\n\n#### Compute time\n\n| Workers trend | Partitions trend | \n|:-------------------------------------:|:-------------------------------------------:|\n![](figures/trends/compute_time_wp_wtrend.png)  |  ![](figures/trends/compute_time_wp_ptrend.png)\n\n\n#### Total time\n\n| Workers trend | Partitions trend | \n|:-------------------------------------:|:-------------------------------------------:|\n![](figures/trends/total_time_wp_wtrend.png)  |  ![](figures/trends/total_time_wp_ptrend.png)\n",
   "metadata": {
    "cell_id": "78e85eddec424098b5b1c534c3bab262",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 4066
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Compute Over Files\nOnce the implementation has been tested over one file to study performances on various combinations of cluster parameter we studied how to run the computation over multiple files.  \nIn fact, the dataset is composed by 1002 raw data training files in `.npz` format, each one containing 5000 simulated events. This means that roughly 700 MB of RAM are occupied to load each file.\n\nTo deal with these large data volumes we developed two types of implementations, **eager** and **lazy**.\n\n### Eager implementation\nThis method assigns the Scheduler the job of reading the data file and partition and send the events to the Workers. A simple `for` loop over the files is enough.  \nAs soon as the Bag containing data is declared the Scheduler starts reading the file and once it has sent data to Workers its RAM is freed, so that it can load the next file once the Workers have finished their jobs.  \n\nParticular care must be given to saving the results: as the images obtained from a single raw data file occupy almost 2.3 GB, retrieving them from Workers to the Scheduler is inefficient and can cause the RAM to fill and the Python kernel to die.  \nThus, each Worker has to take care of saving its own images. This is achieved by exploiting `dask.bag.map_partitions`, taking care of defining the right amount of partitions.",
   "metadata": {
    "cell_id": "63114278176d449cb25adfc8f40e4460",
    "tags": [],
    "owner_user_id": "f20f2658-9eb4-4894-888f-9b0abf9b214c",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 440
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cbaf98f343ba4f30bed323dfcb05b7a4",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 372
   },
   "source": "# Load data from .npz file, taking care of deleting useless variables\ndef load(path):\n    file = np.load(path, allow_pickle=True)\n    data = file['a']\n    del file.f\n    file.close()\n    del file\n    return data\n\n# Given a partition, which is a list containing processed images, save it in a single .npy file\ndef save_partition(chunk, filename):\n    array = np.stack(chunk)\n    np.save(filename[0], array, allow_pickle=False)\n    length = len(array)\n    del array\n    del chunk\n    # the length of the saved array, which is the number of events, is returned for debug purposes\n    return length",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0092f76d1826466abdcced04fa26d536",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 732
   },
   "source": "compute_times = []\nload_times    = []\n\nfor file_name in name_list:\n  \n    # load one file, this is done eagerly\n    start   = time()\n    data_db = db.from_sequence(load_bag(data_folder + file_name, Nevents=None), npartitions=n_workers)\n    stop    = time()\n    load_times.append(stop-start)\n    \n    # declare #npartitions processed file names for each raw data file\n    ofilenames = db.from_sequence(['/root/data/test/proj_' + file_name.split('.')[0] + \n                                   '.part' + str(i) for i in range(n_workers)], npartitions=n_workers)\n\n    # rotate the events, lazily\n    rotated = db.map(rotate_ev, data_db)\n    # mapping the events, lazily\n    mapped = db.map(mapping_single_event, rotated)\n    # save each partition to a single .npy file\n    save = db.map_partitions(save_partition, mapped, ofilenames)\n    \n    # trigger the computation\n    start = time()\n    save.compute()\n    stop  = time()\n    compute_times.append(stop-start)\n\n    # free RAM: have to delete all futures\n    del mapped\n    del rotated\n    del data_db\n    del ofilenames\n\ncompute_times = np.asarray(compute_times)\nrestart_times = np.asarray(restart_times)\n# visualize DAG\nsave.visualize()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(f'Compute times\\n\\tMean:\\t{compute_times.mean():.2f} s', \n      f'\\n\\tstd:\\t{compute_times.std():.2f} s', '\\n', sep='', end='\\n')\n\nprint(f'Restart times\\n\\tMean:\\t{restart_times.mean():.2f} s', \n      f'\\n\\tstd:\\t{restart_times.std():.2f} s', '\\n', sep='', end='\\n')",
   "metadata": {
    "cell_id": "d9c2646ad8624034be6997ba67dad134",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 138
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Lazy implementation\nAs the loading time of one file is comparable to the processing time, we may want to reduce the former, in order to avoid idle time. Exploiting the NFS we want to assign to Workers the job of reading raw data files, aiming to load more than one file at the same time.  \n\nThis can be achieved using `dask.bag.from_delayed` and `dask.delayed`. In fact, if the data Bag is created on the basis of delayed objects, raw data files are read by Workers when the computation is triggered.  \nDue to limitations in the resources available we cannot load and process more than 3 files at a time. Assuming to have more resources available and thus to be able to load and process 10 or more files at a time, the advantages of this method become evident.",
   "metadata": {
    "cell_id": "c5a87a105951472082dffcea84b8ca9b",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 224
   }
  },
  {
   "cell_type": "code",
   "source": "# optimize load_bag function and make it lazy\ndef load_bag(path, Nevents):\n    data_np = load(path)\n    data_mask = data_np[:, :Nevents]\n    bag = [np.vstack([ data_mask[j, i] for j in range(3)]) for i in range(data_mask.shape[1])]\n    del data_mask\n    del data_np\n    return bag \nlazy_load_bag = dask.delayed(load_bag)",
   "metadata": {
    "cell_id": "b3fe1819647c47af8e725517e1f49718",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 210
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "nfiles_cycle = 2\ncompute_times_lz = []\nrestart_times_lz = []\nfor i in range(0, nfiles, nfiles_cycle):\n    \n    # lazy loading of raw data files\n    lazy_loaded = db.from_delayed([lazy_load_bag(data_folder + file_name, None) for file_name in name_list[i:i+nfiles_cycle]]).repartition(n_workers_vm*n_threads_wk*4)\n    ofilenames = db.from_sequence(['/root/data/test/proj_' + name_list[k].split('.')[0] + '.part' + str(j) \n                                    for j in range(int(n_workers_vm*n_threads_wk*4/nfiles_cycle)) \n                                    for k in range(i, i+nfiles_cycle)])\n\n    # process events, lazily\n    rotated = db.map(rotate_ev, lazy_loaded)\n    mapped = db.map(mapping_single_event, rotated)\n    del rotated\n\n    save = db.map_partitions(save_partition, mapped, ofilenames)     \n    del mapped      \n    del lazy_loaded\n    del ofilenames\n    \n    # trigger asynchronous computation\n    start = time()\n    future = client.compute(save, optimize_graph=False)\n    result = client.gather(future)\n    stop = time()\n    compute_times_lz.append(stop-start)\n    \n    start = time()\n    client.restart()\n    stop = time()\n    restart_times_lz.append(stop-start)\n\ncompute_times_lz = np.asarray(compute_times)\nrestart_times_lz = np.asarray(restart_times)\n\nsave.visualize(optimize_graph=False)",
   "metadata": {
    "cell_id": "081ee3a61ea24e22b1b4fec1f075669e",
    "tags": [],
    "owner_user_id": "5e4dbb80-7137-44c2-8f26-b3d7f6482042",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 714
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(f'Compute times\\n\\tMean:\\t{compute_times_lz.mean():.2f} s', \n      f'\\n\\tstd:\\t{compute_times_lz.std():.2f} s', '\\n', sep='', end='\\n')\n\nprint(f'Restart times\\n\\tMean:\\t{restart_times_lz.mean():.2f} s', \n      f'\\n\\tstd:\\t{restart_times_lz.std():.2f} s', '\\n', sep='', end='\\n')",
   "metadata": {
    "cell_id": "30685885467047aaafdb09be7a5a8b8c",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 138
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Conclusions",
   "metadata": {
    "cell_id": "7887cc33597f4362851aa14a781b06d9",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "markdown",
   "source": "# The Città Romanze nickname secret",
   "metadata": {
    "cell_id": "1020568397be4c339148df9fd495aeb9",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 82
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f84d08877cf847fbb348f92dbfdfa576",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4acd5203",
    "execution_start": 1657445566157,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "def list_to_string(s):\n    new = \"\"\n    for x in s:\n        new += x\n    return new",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "58ffc684546f4e34aba5c085a6dc2068",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b414076d",
    "execution_start": 1657445936770,
    "execution_millis": 4,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 190,
    "deepnote_output_heights": [
     21
    ]
   },
   "source": "SEED = 143602177\n\ncitta_romanze      = \"cittaromanze\"\ncitta_romanze_list = [char for char in citta_romanze]\ncitta_romanze_list",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 7,
     "data": {
      "text/plain": "['c', 'i', 't', 't', 'a', 'r', 'o', 'm', 'a', 'n', 'z', 'e']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8a4f80ef41b54f0997b0a49981138809",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "efead02a",
    "execution_start": 1657445606202,
    "execution_millis": 6,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 172,
    "deepnote_output_heights": [
     21
    ]
   },
   "source": "import numpy as np\nnp.random.seed(SEED)\nnp.random.shuffle(citta_romanze_list)\nlist_to_string(citta_romanze_list)",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 6,
     "data": {
      "text/plain": "'marcozanetti'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "cell_id": "38666bdc02b14277975bf17d60b7e548",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 66
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fd23f34a-6533-4f41-81d4-87ad00d37fa2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "266af003-086e-4ecf-834b-5e39bdbef17d",
  "deepnote_execution_queue": []
 }
}